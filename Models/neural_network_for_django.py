# -*- coding: utf-8 -*-
"""Neural Network For Django.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14u7vORJiScdeGxtE3y1_I66sJx9u5_bf
"""

#Imports
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.utils import plot_model
from tensorflow.keras.regularizers import l2
from tensorflow.keras.regularizers import l1
from tensorflow.keras import optimizers
import tensorflow as tf
import math
import random
import os
from keras.models import load_model

# Constants (Feel free to tweek these and observe different outcomes)
ENABLE_DISTRICT_SPLIT = False #This will ensure that schools in the same district are groupped into either trianing, validation or test. Without it they may stradle this
RANDOM_STATE = 10 #This allows for the randomization to be repeatable (can be changed to None to be truely random)
NUM_JOBS = 2 #Number of threads to run on
ENABLE_TPU = False
from numpy.random import seed
seed(RANDOM_STATE)
from tensorflow import set_random_seed
set_random_seed(RANDOM_STATE)

#Loading the data with pandas
math_scores = pd.read_csv("../DB_Export/math.csv")
read = pd.read_csv("../DB_Export/read.csv")
business = pd.read_csv("../DB_Export/BUSSINES.csv")
geo = pd.read_csv("../DB_Export/geo.csv", low_memory=False)
funding = pd.read_csv("../DB_Export/FUNDING.csv")
income = pd.read_csv("../DB_Export/income.csv")
teachers = pd.read_csv("../DB_Export/Teacher Ratios.csv")
title1 = pd.read_csv("../DB_Export/title1.csv")
#Set indexs
math_scores = math_scores.set_index("math_ncessch")
read = read.set_index("read_ncessch")
business = business.set_index("zip_code")
funding["ncesid"] = pd.to_numeric(funding["ncesid"], errors="coerce")
funding = funding.set_index("ncesid")
geo = geo.set_index("ncessch")
income = income.set_index("zipcode")
teachers["ncessch"] = teachers["ncessch"].astype(np.int64)
teachers = teachers.set_index("ncessch")
title1["ncessch"] = title1["ncessch"].astype(np.int64)
title1 = title1.set_index("ncessch")

#Fix title1 data
title1 = title1[title1['title_i_status'] != 'M']
title1 = title1[title1['title_i_status'] != '-9']
title1 = title1[title1['title_i_eligibility'] != 'Missing']
title1 = title1[title1['title_i_eligibility'] != '-9']
title1 = title1[title1['ntnl_school_lunch_program_status'] != 'MISSING']
title1 = title1[title1['ntnl_school_lunch_program_status'] != '-9']
title1 = title1[title1['school_wide_title_i_eligibility'] != 'Missing']
title1 = title1[title1['school_wide_title_i_eligibility'] != '-9']
title1['ntnl_school_lunch_program_status_Yes'] = title1['ntnl_school_lunch_program_status'].apply(lambda x: 'Yes' in x)
title1 = pd.concat([title1.drop('title_i_eligibility',1),pd.get_dummies(title1["title_i_eligibility"], prefix='title_1_eligbility_')], axis=1)
title1 = pd.concat([title1.drop('school_wide_title_i_eligibility',1),pd.get_dummies(title1["school_wide_title_i_eligibility"], prefix='school_wide_title_i_eligibility')], axis=1)
title1 = pd.concat([title1.drop('ntnl_school_lunch_program_status',1),pd.get_dummies(title1["ntnl_school_lunch_program_status"], prefix='ntnl_school_lunch_program_status')], axis=1)

mathStatesPairs = {'HAWAII':0,
    'IOWA':1,
    'RHODE ISLAND':2,
    'OREGON':3,
    'OKLAHOMA':4,
    'OHIO':5,
    'MISSOURI':6,
    'NEW YORK':7,
    'WEST VIRGINIA':8,
    'VIRGINIA':9,
    'MASSACHUSETTS':10,
    'TEXAS':11,
    'ALABAMA':12,
    'SOUTH DAKOTA':13,
    'ARIZONA':14,
    'NEW HAMPSHIRE':15,
    'MICHIGAN':16,
    'MISSISSIPPI':17,
    'DELAWARE':18,
    'MONTANA':19,
    'WYOMING':20,
    'INDIANA':21,
    'SOUTH CAROLINA':22,
    'WISCONSIN':23,
    'VIRGIN ISLANDS':24,
    'NEW MEXICO':25,
    'KENTUCKY':26,
    'KANSAS':27,
    'VERMONT':28,
    'WASHINGTON':29,
    'DISTRICT OF COLUMBIA':30,
    'NORTH DAKOTA':31,
    'NORTH CAROLINA':32,
    'NEBRASKA':33,
    'MARYLAND':34,
    'MAINE':35,
    'MINNESOTA':36,
    'BUREAU OF INDIAN AFFAIRS':37,
    'ILLINOIS':38,
    'FLORIDA':39,
    'NEVADA':40,
    'NEW JERSEY':41,
    'IDAHO':42,
    'CALIFORNIA':43,
    'ALASKA':44,
    'COLORADO':45,
    'CONNECTICUT':46,
    'UTAH':47,
    'ARKANSAS':48,
    'GEORGIA':49,
    'PENNSYLVANIA':50,
    'LOUISIANA':51,
    'PUERTO RICO':52,
    'TENNESSEE':53}

#Fix teachers
teachers= teachers[teachers['num_full_time'] >0]

#Combines the datasets together. This is similar to SQL's join statements but uses Pandas
combined = math_scores[['math_leaid','math_stnam','math_all_grades_numvalid','math_all_grades_pctprof_low','math_all_grades_pctprof_high']].join([read[['read_all_grades_numvalid','read_all_grades_pctprof_low','read_all_grades_pctprof_high']],geo[['zip','locale','lat','lon']]], how="inner")
#combined = combined.join(business[['num_establishments',"num_paid_employees", "first_quarter_payroll", "annual_payroll"]], how="inner", on="zip")
combined = combined.join(funding.drop(["idcensus","name","conum","csa","cbsa","enroll"],1), how="inner", on="math_leaid")
combined = combined.join(income.drop(["statefips",'state'], 1), how="inner", on="zip")
combined = combined.join(teachers["num_full_time"], how="inner")
combined = combined.join(title1[["title_i_status",'ntnl_school_lunch_program_status_Yes', 'title_1_eligbility__No',
       'title_1_eligbility__Not Applicable', 'title_1_eligbility__Yes',
       'school_wide_title_i_eligibility_No',
       'school_wide_title_i_eligibility_Not Applicable',
       'school_wide_title_i_eligibility_Yes',
       'ntnl_school_lunch_program_status_No',
       'ntnl_school_lunch_program_status_Yes, participating without using any Provision or the CEO',
       'ntnl_school_lunch_program_status_Yes, under Community Eligibility Option (CEO)',
       'ntnl_school_lunch_program_status_Yes, under Provision 1',
       'ntnl_school_lunch_program_status_Yes, under Provision 2']], how="inner")
combined = combined.drop(['zip'],1)
combined = combined[combined['math_all_grades_pctprof_low'] >=0]
combined = combined[combined['read_all_grades_pctprof_low'] >=0]
combined['math_full_time'] = combined['math_all_grades_numvalid']/combined['num_full_time']
combined['read_full_time'] = combined['read_all_grades_numvalid']/combined['num_full_time']

def convertStatesToNum(i):
    return mathStatesPairs[i] 
combined['stnam'] = combined['math_stnam'].apply(convertStatesToNum)
combined = combined.drop('math_stnam', 1)
len(combined)

usedFeatures = {
    'lon' : 'SCALED',
    'math_all_grades_numvalid':'SCALED_LOG',
    'lat':'SCALED',
    'pct_title1':'SCALED_LOG',
    'read_full_time':'SCALED_LOG',
    'num_full_time':'SCALED_LOG',
    'before_credits_amount':'SCALED_LOG',
    'stnam':'CAT',
    'ntnl_school_lunch_program_status_Yes, participating without using any Provision or the CEO':'NONE',
    'excess_income_credit_amount': 'SCALED_LOG',
    'num_of_head_house_returns':'SCALED_LOG',
    'pct_charges':'SCALED_LOG',
    'pct_parent_gov_cont':'SCALED',
    'pct_rev_locale':'SCALED',
    'pcts_gen_form':'SCALED',
}
def Scaler(series):
  scaler = StandardScaler()
  scaler.fit(series.values.reshape(-1,1))
  return scaler.transform(series.values.reshape(-1,1)).reshape(-1,1)
for i in usedFeatures.keys():
  if usedFeatures[i] == 'SCALED':
    combined[i] = Scaler(combined[i]) 
  elif usedFeatures[i] == 'SCALED_LOG':
    combined[i] = combined[i].apply(lambda x:np.log(x) if x != 0 else 0 )


def prepareForKeras(dataset):
  cols = dataset['school_wide_title_i_eligibility_Yes'].values
  cols = cols.reshape([len(cols),1])
  for i in usedFeatures.keys():
    if usedFeatures[i] == 'CAT':
      cols = np.concatenate([cols, to_categorical(dataset[i].values)], axis=1)
    else:
      cols = np.concatenate([cols, dataset[i].values.reshape([-1,1])], axis=1)
  return cols
#input_num = prepareForKeras(math_X_test).shape[1]
#print(input_num)

def delta(x, y):
  return abs(x-y)
def createModel(X_train, y_train, X_val, y_val):
  optimizer = optimizers.Adam(lr=.0001)
  model = Sequential()
  model.add(Dense(200, activation='relu', input_dim=input_num,kernel_regularizer = l2(.05)))
  model.add(Dropout(rate=.5))
  model.add(Dense(200, activation='relu', kernel_regularizer = l2(.05)))
  model.add(Dropout(rate=.5))
  model.add(Dense(200, activation='relu', kernel_regularizer = l2(.05)))
  model.add(Dense(1))
  model.compile(loss='mean_squared_error',
              optimizer=optimizer,
              metrics=['accuracy', delta])
  model = convertForTPU(model)
  train  =[0]
  train_delta = [0]
  val = [0]
  val_delta = [0]
  epochs = 1
#math_model.fit(prepareForKeras(math_X_train), math_y_train,  epochs=50, batch_size=128, shuffle=True, verbose=1)
  while abs(train[-1]-val[-1]) < .02:
    history = model.fit(prepareForKeras(X_train), y_train,  epochs=1, batch_size=8, shuffle=True, verbose=0)
    train.append(r2_score(np.array(y_train), model.predict(prepareForKeras(X_train))))
    val.append(r2_score(np.array(y_val), model.predict(prepareForKeras(X_val))))
    train_delta.append(history.history['delta'][-1])
    val_delta.append(np.mean(delta(np.array(y_val), model.predict(prepareForKeras(X_val)))))
    if epochs % 20 == 0:
      print(epochs)
      print("Train", train[-1])
      print("val", val[-1])
      print("Train Delta", train_delta[-1])
      print("Val Delta", val_delta[-1])
      print("Delta Delta", abs( train_delta[-1] - val_delta[-1]))
      print("Delta", abs(train[-1]-val[-1]))
    epochs += 1
  print("Math train R^2", r2_score(np.array(y_train), model.predict(prepareForKeras(X_train))))
  print("Math val R^2", r2_score(np.array(y_val), model.predict(prepareForKeras(X_val))))
  plt.plot(val)
  plt.plot(train)
  plt.xlabel = "epoch"
  plt.ylabel = "R^2"
  plt.legend(("Train", "Validation"))
  plt.show()
  return model
#read_model = createModel(read_X_train, read_y_train, read_X_val, read_y_val)
